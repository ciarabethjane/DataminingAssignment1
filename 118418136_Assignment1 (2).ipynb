{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "118418136_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ciara Elizabeth Griffin 118418136**\n",
        "\n",
        "**Part 1 - Preparing the dataset**\n",
        "\n",
        "**1. Load the dataset**\n",
        "\n",
        "> In this part, I import both pandas and numpy. I create a variable to hold the path to the csv file I want to load in as a data set, having added \"?raw=true\" to the end of the url so that the programme can read it in. I then use the .read_csv() method to load the dataset.\n",
        "\n",
        "\n",
        "**2. Display the attributes names and types**\n",
        "\n",
        ">  I use a print function and .dtypes to print out the names and types of the attributes in the dataset.\n",
        "\n",
        "**3. Delete the columns PassengerID and Name**\n",
        "\n",
        ">  Using the .drop() method, I remove PassengerID and Name columns from the dataset using the parameter 'axis 1' as in a pandas dataframe, 'axis 1' refers to the columns whereas 'axis 0' refers to the rows.\n",
        "\n",
        "**4. Replace all missing values with 0**\n",
        "\n",
        "  > The .fillna() method in pandas replaces null values with a value that's passed as a parameter. In this case, I set that parameter to 0.\n",
        "\n",
        "**5. Transform the Sex column into a numerical one**\n",
        "\n",
        "  >  My solution for this part of the problem consists of code that I adapted and extended from https://bit.ly/36L8DdO and https://bit.ly/3IBYq1z . First, I replaced values that are labelled as male with the integer 1, and I then replaced values that are labelled as female with the integer 2. I passed the parameter 'inplace = True' because .replace isn't an inplace operation by default, which means it needs to be assigned back to the existing dataset in order for it to have an effect. I then use a print function to check the data types to confirm that Sex is now an integer type\n",
        "\n",
        "**6. Use Survived as the target label and the rest of the data frame as features**\n",
        "\n",
        "  >  I based my solution for this part of the problem on the lab example given in the CS3033 Lab on 01/03/2022. Here the .iloc method is used to select the \"Survived\" column using the column's position. The column is held in the variable \"targetLabel\". I then print \"targetLabel\" to check that it only contains the \"Survived\" column. \n",
        "  \n",
        "  > To use the rest of the dataframe as features, I use the .drop() method on the dataframe to remove the \"Survived\" column from it. I then print the \"features\" variable to check that the \"Survived\" column has been removed.\n",
        "\n",
        "**7. Divide your dataset in 80% for training and 20% for test**\n",
        "\n",
        " > I extended and adapted code from https://bit.ly/3tkKeDK for this part of the solution. First I work out how many rows should be in each set by dividing the length of the dataframe by 100 and then multiplying it by 80 and 20 respectively. I use typecasting to ensure that the resulting numbers are integers and not floats. I then use the head and tail methods to divide the dataset into training and test sets.\n",
        "\n",
        "**8. Scale the columns using min-max scalers**\n",
        " > I adapted code from https://bit.ly/3HB6CgY which uses the mathematical formula for min-max scaling to scale the dataframe. The formula I used was:\n",
        " \n",
        " > x_scaled = (x - min(x))/(max(x)-min(x))\n",
        "\n",
        "**9. Print the shape of the train and test sets**\n",
        "\n",
        " > Using print statements and the .shape method, the program prints the shape of both the training and test datasets."
      ],
      "metadata": {
        "id": "Altbp3reEzLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Part 1\n",
        "path = \"https://bit.ly/3vFn3qi?raw=true\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Part 2\n",
        "print(df.dtypes)\n",
        "\n",
        "# Part 3\n",
        "df = df.drop([\"PassengerId\", \"Name\"], axis=1)\n",
        "\n",
        "# Part 4\n",
        "df = df.fillna(0)\n",
        "\n",
        "# Part 5\n",
        "df[\"Sex\"].replace(\"male\", 1, inplace=True)\n",
        "df[\"Sex\"].replace(\"female\", 2, inplace=True)\n",
        "print(df.dtypes)\n",
        "\n",
        "# Part 6\n",
        "targetLabel = df.iloc[:,0:1]\n",
        "print(\"targetLabel\")\n",
        "#print(targetLabel)\n",
        "\n",
        "features = df.drop(\"Survived\", axis=1)\n",
        "#print(features)\n",
        "\n",
        "\n",
        "# Part 7\n",
        "setDivision80 = int((len(df)/100) * 80) + 1\n",
        "trainSet = df.iloc[:setDivision80,:]\n",
        "testSet = df.iloc[setDivision80:,:]\n",
        "\n",
        "# Part 8\n",
        "trainSet_scaled = (trainSet - trainSet.min(axis=0)) / (trainSet.max(axis=0) - trainSet.min(axis=0))\n",
        "testSet_scaled = (testSet - testSet.min(axis=0)) / (testSet.max(axis=0) - testSet.min(axis=0))\n",
        "#Split target label into training and test sets for convenience later\n",
        "targetLabelTrainSet = targetLabel.iloc[:setDivision80]\n",
        "targetLabelTestSet = targetLabel.iloc[setDivision80:]\n",
        "print(len(targetLabelTrainSet))\n",
        "print(len(targetLabelTestSet))\n",
        "# Part 9\n",
        "print(trainSet_scaled.shape)\n",
        "print(testSet_scaled.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKnp7GwAE_fJ",
        "outputId": "0fb25139-41cb-4e1d-9768-d28154b4b893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId                  int64\n",
            "Survived                     int64\n",
            "Pclass                       int64\n",
            "Name                        object\n",
            "Sex                         object\n",
            "Age                        float64\n",
            "Siblings/Spouses Aboard      int64\n",
            "Parents/Children Aboard    float64\n",
            "Fare                       float64\n",
            "dtype: object\n",
            "Survived                     int64\n",
            "Pclass                       int64\n",
            "Sex                          int64\n",
            "Age                        float64\n",
            "Siblings/Spouses Aboard      int64\n",
            "Parents/Children Aboard    float64\n",
            "Fare                       float64\n",
            "dtype: object\n",
            "targetLabel\n",
            "710\n",
            "177\n",
            "(710, 7)\n",
            "(177, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2 - k-NN implementation**\n",
        "\n",
        "**1. Calculating the Euclidean Distance**\n",
        "\n",
        "\n",
        "2. Compute the accuracy and plot the confusion matrix\n",
        "\n",
        "  a. Briefly describe what the confusion matrix shows us"
      ],
      "metadata": {
        "id": "iLvo-pRihMsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the Euclidean Distance between two points\n",
        "def EuclideanDist(queryRow, queryColIndex, row2):\n",
        "    # Create variables to hold the values of each point\n",
        "    value1 = float(row2[queryColIndex])\n",
        "    value2 = float(queryRow[queryColIndex])\n",
        "    # create a distance variable that calculates (value1-value2)**2\n",
        "    distance = (value1 - value2)**2\n",
        "    # return the square root of the distance variable (the Euclidean Distance between two points)\n",
        "    return np.sqrt(distance)\n"
      ],
      "metadata": {
        "id": "kDm7Zx82iHqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2.2 KNN Implementation**\n",
        "  1. Calculate the Euclidean distance between the query point (each point in the testing set) and all the training points of the training set.\n",
        "\n",
        "  2. Then pick the k points with the smallest distance to the query point (k must be a hyperparameter)\n",
        "\n",
        "  3. Select the most common class among the k points"
      ],
      "metadata": {
        "id": "mkI2m4BuT8KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to implement the kNN\n",
        "def kNNImplementation(hyperparameter, trainingSet, targetLabelPassed, queryRow):\n",
        "   # Create an array to store the result of each distance calculation\n",
        "  distancesCalculated = []\n",
        "  # Convert each dataframe into array format to make it easier to cycle through/pass values into EuclideanDist function\n",
        "  trainSetArray = np.array(trainingSet.to_numpy())\n",
        "  queryRowArray = np.array(queryRow.to_numpy())\n",
        "  # Create a counter variable to be used as an identifier in the array as to what distance belongs to what training point\n",
        "  trainRowCounter = 0\n",
        " \n",
        "  # Initiate a loop to cycle through the query points\n",
        "  for trainRows in trainSetArray:\n",
        "    trainRowArray = np.array(trainSetArray[trainRowCounter])\n",
        "    # Create a counter variable to count through the columns of each row\n",
        "    queryColCounter = 0\n",
        "    # Cycle through the columns in each row using a for loop\n",
        "    for cols in queryRowArray:\n",
        "      # calculate the euclidean distance between the query point and training point\n",
        "      pointsDistance = EuclideanDist(queryRow, queryColCounter, trainSetArray[trainRowCounter])\n",
        "      # store the result of that calculation and the training point it belongs to\n",
        "      arrayToStore = [trainRowCounter, pointsDistance] \n",
        "      # add the arrayToStore variable to the list of distances calculated\n",
        "      distancesCalculated.append(arrayToStore)\n",
        "      # Increment the column counter\n",
        "      queryColCounter = queryColCounter + 1\n",
        "    # Increment the training row counter\n",
        "    trainRowCounter = trainRowCounter + 1\n",
        "  # Once the loop has cycled through each training point:\n",
        "  # sort the training points in increasing order of distance\n",
        "  distancesSorted = list(sorted(distancesCalculated, key=lambda x:x[1]))\n",
        "  #   select k number of the points\n",
        "  selectedPoints = distancesSorted[0:hyperparameter]\n",
        "  \n",
        "  # determine which class is most common amongst the k points\n",
        "  itemNo = 0 \n",
        "  rowsInQuestion = []\n",
        "  # Cycle through the list of selectedPoints using a for loop\n",
        "  for x in selectedPoints:\n",
        "    # establish what row the point is from (Row number is held at index 0 of each list containing row number and distance calculated)\n",
        "    rowValue = selectedPoints[itemNo][0]\n",
        "    # increment the itemNo counter\n",
        "    itemNo = itemNo + 1\n",
        "    # Add the row number to the rowsInQuestion list to be used later to determine the actual class of the row\n",
        "    rowsInQuestion.append(rowValue)\n",
        "\n",
        "  # Create a list containing the target labels of the test set\n",
        "  targetLabelTrainSetArray = list(targetLabelPassed.to_numpy().squeeze())\n",
        "  # Create an empty list to hold the classes of the test set\n",
        "  classesOfRows = []\n",
        "  # Create a counter variable\n",
        "  y = 0\n",
        "  # while loop to cycle through the rowsInQuestion list\n",
        "  while y < len(rowsInQuestion):\n",
        "    # Use the row numbe contained at index y of the rowsInQuestion list to retrieve the class from the targetLabel list\n",
        "    classInQuestion = int(targetLabelTrainSetArray[rowsInQuestion[y]])\n",
        "    # Add the class to the classesOfRows list\n",
        "    classesOfRows.append(classInQuestion)\n",
        "    # Increment the counter variable\n",
        "    y = y + 1\n",
        "\n",
        "  # return the most common class as the class of the query point\n",
        "  # code adapted from https://www.delftstack.com/howto/python/mode-of-list-in-python/\n",
        "  commonClassInt = (max(set(classesOfRows), key = classesOfRows.count))\n",
        "  return commonClassInt\n",
        " "
      ],
      "metadata": {
        "id": "qJD6fKPMUDUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2.3 - Calculating the Accuracy and the Confusion Matrix**"
      ],
      "metadata": {
        "id": "tNBKg5i3UNZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# code for accuracy adapted from https://stackoverflow.com/questions/66187861/finding-the-accuracy-from-the-confusion-matrix-in-pd-crosstab\n",
        "def confusionMatrix(actuclass, predclass, currentHP):\n",
        "    # Calculate the confusion Matrix using the pandas crosstab function\n",
        "    confusionmatrix = pd.crosstab(index=np.array(actuclass), columns=np.array(predclass), rownames=['Actual'], colnames=['Predicted'])\n",
        "    # Calculate the accuracy of the kNN \n",
        "    accuracy = np.diag(confusionmatrix).sum() / confusionmatrix.to_numpy().sum()\n",
        "    print(\"Confusion Matrix\", currentHP)\n",
        "    print(confusionmatrix)\n",
        "    print(\"Accuracy of Hyperparameter: \", round(accuracy*100), \"%\")\n",
        "    print(\"\")\n",
        "  \n"
      ],
      "metadata": {
        "id": "fG3SWJuuUUu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3 - Hyperparameters Search**\n",
        "\n",
        "**Part 3.1 Test [1, 3, 5, 7, 9, 11] as possible k  values**"
      ],
      "metadata": {
        "id": "Z_wKFOUKPWRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an array to hold the hyperparameters to be tested\n",
        "hyperparameters = [1, 3, 5, 7, 9, 11]\n",
        "# Create a counter variable to hold the index value of the current hyperparameter being used\n",
        "hyperParameterIndex = 0\n",
        "# Cycle through each item in hyperparameters using a for loop\n",
        "for i in hyperparameters:\n",
        "  # Create a list to store the predicted class of each test case\n",
        "  predclass = []\n",
        "  # Create a list to store the index of each row to be used to retrieve the actual class of each test case\n",
        "  rows = []\n",
        "  # Create a counter variable to hold the index of the current row\n",
        "  rowCounter = 0\n",
        "  # Create a variable that specifies which hyperparameter to use\n",
        "  currentHP = hyperparameters[hyperParameterIndex]\n",
        "  # Cycle through each row in the test set using a for loop\n",
        "  while rowCounter < len(testSet_scaled):\n",
        "    # Call the kNNImplementation function to predict which class the test case belongs to\n",
        "    hyperParameter = kNNImplementation(currentHP, trainSet_scaled, targetLabelTrainSet, testSet_scaled.iloc[rowCounter])\n",
        "    # Add the predicted class to the list of predicted classes\n",
        "    predclass.append(hyperParameter)\n",
        "    # Add the row's index to the list of rows' indices\n",
        "    rows.append(rowCounter)\n",
        "    # Increment the rowCounter variable\n",
        "    rowCounter = rowCounter + 1\n",
        "    # Create an array containing the test set's actual classes\n",
        "    targetLabelTest = targetLabelTestSet.to_numpy().squeeze()\n",
        "    # Convert it into a list\n",
        "    targetLabelTestArray = list(np.array(targetLabelTest))\n",
        "    # Create an empty list to hold the actual classes of the rows in question\n",
        "    actuclass = []\n",
        "    # Create another counter variable\n",
        "    z = 0\n",
        "  # Cycle through the list containing the rows' indices\n",
        "  for items in rows:\n",
        "    # Create a variable to hold the actual class of the relevant row\n",
        "    value = targetLabelTestArray[rows[z]]\n",
        "    # Add that value to the list of actual classes\n",
        "    actuclass.append(value) \n",
        "    # Increment the Counter variable for the rows list indexes\n",
        "    z = z + 1\n",
        "  # Call the confusionMatrix function to calculate the confusionMatrix and accuracy of the current hyperparameters\n",
        "  confusionMat = confusionMatrix(actuclass, predclass, currentHP)\n",
        "  # Increment the counter variable for the current hyperParameterIndex\n",
        "  hyperParameterIndex = hyperParameterIndex + 1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xGMpqNV_-cP",
        "outputId": "c87d9946-a1af-4eb5-832e-9a6b227c2f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix 1\n",
            "Predicted    0  1\n",
            "Actual           \n",
            "0          114  0\n",
            "1           57  6\n",
            "Accuracy of Hyperparameter:  68 %\n",
            "\n",
            "Confusion Matrix 3\n",
            "Predicted    0   1\n",
            "Actual            \n",
            "0          114   0\n",
            "1           32  31\n",
            "Accuracy of Hyperparameter:  82 %\n",
            "\n",
            "Confusion Matrix 5\n",
            "Predicted    0   1\n",
            "Actual            \n",
            "0          109   5\n",
            "1           10  53\n",
            "Accuracy of Hyperparameter:  92 %\n",
            "\n",
            "Confusion Matrix 7\n",
            "Predicted   0   1\n",
            "Actual           \n",
            "0          74  40\n",
            "1           0  63\n",
            "Accuracy of Hyperparameter:  77 %\n",
            "\n",
            "Confusion Matrix 9\n",
            "Predicted   0   1\n",
            "Actual           \n",
            "0          34  80\n",
            "1           1  62\n",
            "Accuracy of Hyperparameter:  54 %\n",
            "\n",
            "Confusion Matrix 11\n",
            "Predicted   0   1\n",
            "Actual           \n",
            "0          88  26\n",
            "1           5  58\n",
            "Accuracy of Hyperparameter:  82 %\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3.2 - Select the best one and explain why**\n",
        "\n",
        "Based on the confusion matrix calculated for each hyperparameter:\n",
        "\n",
        "* Hyperparameter of 1 has an accuracy of 68%\n",
        "* Hyperparameter of 3 has an accuracy of 82%\n",
        "* Hyperparameter of 5 has an accuracy of 92%\n",
        "* Hyperparameter of 7 has an accuracy of 77%\n",
        "* Hyperparameter of 9 has an accuracy of 54%\n",
        "* Hyperparameter of 11 has an accuracy of 82%\n",
        "\n",
        "The hyperparameters of 5 has the best accuracy percentage on the test cases (92% of test cases predicted correctly) making it the best hyperparameter out of the ones tested on this model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F64NpJI9Mgti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4 - Weighted kNN**\n",
        "1. Create a second version of your k-NN approach that implements a distance\n",
        "weighted k-NN algorithm. Use the inverse distance squared as your distance\n",
        "weighted metric\n"
      ],
      "metadata": {
        "id": "7dkamTojPw11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kNNWeighted(hyperparameter, trainingSet, targetLabelPassed, queryRow):\n",
        "   # Create an array to store the result of each distance calculation\n",
        "  distancesCalculated = []\n",
        "  # Convert each dataframe into array format to make it easier to cycle through/pass values into EuclideanDist function\n",
        "  trainSetArray = np.array(trainingSet.to_numpy())\n",
        "  queryRowArray = np.array(queryRow.to_numpy())\n",
        "  # Create a counter variable to be used as an identifier in the array as to what distance belongs to what training point\n",
        "  trainRowCounter = 0\n",
        " \n",
        "  # Initiate a loop to cycle through the query points\n",
        "  for trainRows in trainSetArray:\n",
        "    trainRowArray = np.array(trainSetArray[trainRowCounter])\n",
        "    # Create a counter variable to count through the columns of each row\n",
        "    queryColCounter = 0\n",
        "    # Cycle through the columns in each row using a for loop\n",
        "    for cols in queryRowArray:\n",
        "      # calculate the euclidean distance between the query point and training point\n",
        "      pointsDistance = EuclideanDist(queryRow, queryColCounter, trainSetArray[trainRowCounter])\n",
        "      # store the result of that calculation and the training point it belongs to\n",
        "      arrayToStore = [trainRowCounter, pointsDistance] \n",
        "      # add the arrayToStore variable to the list of distances calculated\n",
        "      distancesCalculated.append(arrayToStore)\n",
        "      # Increment the column counter\n",
        "      queryColCounter = queryColCounter + 1\n",
        "    # Increment the training row counter\n",
        "    trainRowCounter = trainRowCounter + 1\n",
        "  # Once the loop has cycled through each training point:\n",
        "  # sort the training points in increasing order of distance\n",
        "  distancesSorted = list(sorted(distancesCalculated, key=lambda x:x[1]))\n",
        "  #   select k number of the points\n",
        "  selectedPoints = distancesSorted[0:hyperparameter]\n",
        "\n",
        "  itemNo = 0 \n",
        "  rowsInQuestion = []\n",
        "  # Cycle through the list of selectedPoints using a for loop\n",
        "  for x in selectedPoints:\n",
        "    # establish what row the point is from (Row number is held at index 0 of each list containing row number and distance calculated)\n",
        "    rowValue = selectedPoints[itemNo][0]\n",
        "    # increment the itemNo counter\n",
        "    itemNo = itemNo + 1\n",
        "    # Add the row number to the rowsInQuestion list to be used later to determine the actual class of the row\n",
        "    rowsInQuestion.append(rowValue)\n",
        "\n",
        "  # Create a list containing the target labels of the test set\n",
        "  targetLabelTrainSetArray = list(targetLabelPassed.to_numpy().squeeze())\n",
        "  # Create an empty list to hold the classes of the test set\n",
        "  classesOfRows = []\n",
        "  # Create a counter variable\n",
        "  y = 0\n",
        "  # while loop to cycle through the rowsInQuestion list\n",
        "  while y < len(rowsInQuestion):\n",
        "    # Use the row numbe contained at index y of the rowsInQuestion list to retrieve the class from the targetLabel list\n",
        "    classInQuestion = int(targetLabelTrainSetArray[rowsInQuestion[y]])\n",
        "    arrayToBeSaved = [rowsInQuestion[y], selectedPoints[y][1], classInQuestion]\n",
        "    # Add the class to the classesOfRows list\n",
        "    classesOfRows.append(arrayToBeSaved)\n",
        "    arrayToBeSaved = []\n",
        "    # Increment the counter variable\n",
        "    y = y + 1\n",
        "  \n",
        "  # Calculate Weights\n",
        "  listOfInverse = []\n",
        "  weights = []\n",
        "  sum = 0.0\n",
        "  for i in range(hyperparameter):\n",
        "    distanceWeighting = 1/classesOfRows[i][1]\n",
        "    # if the distanceWeighting has an infinite value, re-assign it a value of 1\n",
        "    # Based on what I could find online, scikit learn handles 1/0 as being equal to 1, which is why I decided to handle it this way\n",
        "    if distanceWeighting == np.inf:\n",
        "      distanceWeighting = 1\n",
        "    # Create a list containing the current row number, the distanceWeighting variable for this row's distance and the class of this row\n",
        "    listOfInverseToAdd = [classesOfRows[i][0], distanceWeighting, classesOfRows[i][2]]\n",
        "    # Add this list to the listOfInverseList\n",
        "    listOfInverse.append(listOfInverseToAdd)\n",
        "    # Increase the sum by the value in distanceWeighting\n",
        "    sum = round((sum + distanceWeighting),2)\n",
        "  # For loop to cycle through the list of Inverse list\n",
        "  for i in range(len(listOfInverse)):\n",
        "    #Square the value of distanceWeighting/total sum value to get the inverse distance squared\n",
        "    sumDivided = (listOfInverse[i][1]/sum)**2\n",
        "    # Use this as the inverse distance for voting for this row. append the list to change the distanceWeighting value to the sumDivided Value\n",
        "    weights.append([listOfInverse[i][0], sumDivided, listOfInverse[i][2]])\n",
        "  sum0 = 0.0\n",
        "  sum1 = 0.0\n",
        "  # Set commonClass to a default of -1, to be used as a debugger tool\n",
        "  commonClass = -1\n",
        "  # For loop to cycle through the weights list\n",
        "  for i in range(len(weights)):\n",
        "    #If the class contained in index 2 of the list contained at index i of the weights list is equal to 0\n",
        "    if weights[i][2] == 0:\n",
        "      # Increase sum0 by the sumDivided value held for this row's distance\n",
        "      sum0 = sum0 + weights[i][1]\n",
        "    #If the class contained in index 2 of the list contained at index i of the weights list is equal to 1\n",
        "    if weights[i][2] == 1:\n",
        "      # Increase sum1 by the sumDivided value held for this row's distance\n",
        "      sum1 = sum1 + weights[i][1]\n",
        "  #Determine commonClass using votes by which class has the heavier weight\n",
        "  if sum0 > sum1:\n",
        "    commonClass = 0\n",
        "  if sum1 > sum0:\n",
        "    commonClass = 1\n",
        "  #Return the commonClass\n",
        "  return commonClass\n",
        "\n",
        "  \n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "qrUNK8CWQF6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing kNN Weighted"
      ],
      "metadata": {
        "id": "6WTK3kSy4yem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = [1,3,5,7,9,11]\n",
        "for i in range(len(hyperparameters)):\n",
        "  currentHP = hyperparameters[i]\n",
        "  predclass = []\n",
        "  rows = []\n",
        "  rowCounter = 0\n",
        "  while rowCounter < len(testSet_scaled):\n",
        "    # Call the kNNImplementation function to predict which class the test case belongs to\n",
        "    hyperParameter = kNNWeighted(currentHP, trainSet_scaled, targetLabelTrainSet, testSet_scaled.iloc[rowCounter])\n",
        "    # Add the predicted class to the list of predicted classes\n",
        "    predclass.append(hyperParameter)\n",
        "    # Add the row's index to the list of rows' indices\n",
        "    rows.append(rowCounter)\n",
        "    # Increment the rowCounter variable\n",
        "    rowCounter = rowCounter + 1\n",
        "    # Create an array containing the test set's actual classes\n",
        "    targetLabelTest = targetLabelTestSet.to_numpy().squeeze()\n",
        "    # Convert it into a list\n",
        "    targetLabelTestArray = list(np.array(targetLabelTest))\n",
        "  # Create an empty list to hold the actual classes of the rows in question\n",
        "  actuclass = []\n",
        "  # Create another counter variable\n",
        "  z = 0\n",
        "  # Cycle through the list containing the rows' indices\n",
        "  for items in rows:\n",
        "    # Create a variable to hold the actual class of the relevant row\n",
        "    value = targetLabelTestArray[rows[z]]\n",
        "    # Add that value to the list of actual classes\n",
        "    actuclass.append(value) \n",
        "    # Increment the Counter variable for the rows list indexes\n",
        "    z = z + 1\n",
        "  # Call the confusionMatrix function to calculate the confusionMatrix and accuracy of the current hyperparameters\n",
        "  confusionMat = confusionMatrix(actuclass, predclass, currentHP)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODux9Fha4045",
        "outputId": "20da0b67-6dc2-43af-fb24-ced2b49edb13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix 1\n",
            "Predicted    0  1\n",
            "Actual           \n",
            "0          114  0\n",
            "1           57  6\n",
            "Accuracy of Hyperparameter:  68 %\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix 3\n",
            "Predicted    0   1\n",
            "Actual            \n",
            "0          114   0\n",
            "1           32  31\n",
            "Accuracy of Hyperparameter:  82 %\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix 5\n",
            "Predicted    0   1\n",
            "Actual            \n",
            "0          109   5\n",
            "1           10  53\n",
            "Accuracy of Hyperparameter:  92 %\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix 7\n",
            "Predicted   0   1\n",
            "Actual           \n",
            "0          74  40\n",
            "1           0  63\n",
            "Accuracy of Hyperparameter:  77 %\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix 9\n",
            "Predicted   0   1\n",
            "Actual           \n",
            "0          34  80\n",
            "1           1  62\n",
            "Accuracy of Hyperparameter:  54 %\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix 11\n",
            "Predicted   0   1\n",
            "Actual           \n",
            "0          88  26\n",
            "1           5  58\n",
            "Accuracy of Hyperparameter:  82 %\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Does the Weighted kNN outperform the non-weighted kNN?** \n",
        "\n",
        "My kNNWeighted function is exactly as accurate as my non-weighted kNN function. I'm assuming this is because of how I chose to handle the infinite value returned when a distance of 0.00 was divided by 1 (i.e. assigning it a value of 1, just as scikit would do). It returns the same accuracy for each of the hyperparameters that were tested with the non-weighted kNN, therefore it doesn't outperform the regular kNN in this case."
      ],
      "metadata": {
        "id": "-tR7JCtXFjHL"
      }
    }
  ]
}